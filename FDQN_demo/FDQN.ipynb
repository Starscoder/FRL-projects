{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from DRL import DRL\n",
    "from DQN import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 3\n",
    "BATCH_SIZE = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 18:57:17,724\tINFO worker.py:833 -- Calling ray.init() again after it has already been called.\n",
      "2021-12-08 18:57:17,732\tWARNING sample.py:403 -- DeprecationWarning: wrapping <function fedrl.<locals>.<lambda> at 0x7fc81e56a598> with tune.function() is no longer needed\n",
      "2021-12-08 18:57:17,733\tWARNING sample.py:403 -- DeprecationWarning: wrapping <function fed_train.<locals>.fed_learn at 0x7fc80dd10c80> with tune.function() is no longer needed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 18:57:17 (running for 00:00:00.14)<br>Memory usage on this node: 5.9/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/2.58 GiB heap, 0.0/1.29 GiB objects<br>Result logdir: /Users/a10.11.5/ray_results/CartPole-v0-PG-5<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_multienv_FedRL_9c0e2_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/a10.11.5/anaconda3/lib/python3.7/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:57:33,537\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:57:35,282\tINFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:57:35,283\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:57:35,283\tINFO trainer.py:772 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/a10.11.5/anaconda3/lib/python3.7/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/a10.11.5/anaconda3/lib/python3.7/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/a10.11.5/anaconda3/lib/python3.7/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/a10.11.5/anaconda3/lib/python3.7/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/a10.11.5/anaconda3/lib/python3.7/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/a10.11.5/anaconda3/lib/python3.7/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/a10.11.5/anaconda3/lib/python3.7/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=33658)\u001b[0m 2021-12-08 18:57:55,460\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=33665)\u001b[0m 2021-12-08 18:57:55,460\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=33668)\u001b[0m 2021-12-08 18:57:55,460\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=33671)\u001b[0m 2021-12-08 18:57:55,460\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=33657)\u001b[0m 2021-12-08 18:57:55,460\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=33659)\u001b[0m 2021-12-08 18:57:55,460\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=33660)\u001b[0m 2021-12-08 18:57:55,461\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[2m\u001b[36m(pid=33658)\u001b[0m 2021-12-08 18:57:57,994\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33665)\u001b[0m 2021-12-08 18:57:58,014\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33668)\u001b[0m 2021-12-08 18:57:58,009\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33671)\u001b[0m 2021-12-08 18:57:57,998\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33657)\u001b[0m 2021-12-08 18:57:57,999\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33659)\u001b[0m 2021-12-08 18:57:58,023\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33660)\u001b[0m 2021-12-08 18:57:58,042\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33657)\u001b[0m 2021-12-08 18:57:58,170\tWARNING deprecation.py:39 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33658)\u001b[0m 2021-12-08 18:57:58,269\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33665)\u001b[0m 2021-12-08 18:57:58,245\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33668)\u001b[0m 2021-12-08 18:57:58,264\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33671)\u001b[0m 2021-12-08 18:57:58,272\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33657)\u001b[0m 2021-12-08 18:57:58,261\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33659)\u001b[0m 2021-12-08 18:57:58,253\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33660)\u001b[0m 2021-12-08 18:57:58,277\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:57:59,313\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:57:59,443\tWARNING deprecation.py:39 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:57:59,524\tWARNING deprecation.py:39 -- DeprecationWarning: `callbacks dict interface` has been deprecated. Use `a class extending rllib.agents.callbacks.DefaultCallbacks` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,036\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,036\tINFO trainable.py:113 -- Trainable.setup took 24.757 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,047\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 18:58:00 (running for 00:00:42.37)<br>Memory usage on this node: 6.4/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 8.0/8 CPUs, 0/0 GPUs, 0.0/2.58 GiB heap, 0.0/1.29 GiB objects<br>Result logdir: /Users/a10.11.5/ray_results/CartPole-v0-PG-5<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_multienv_FedRL_9c0e2_00000</td><td>RUNNING </td><td>127.0.0.1:33654</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 18:58:00,142\tERROR trial_runner.py:924 -- Trial PG_multienv_FedRL_9c0e2_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 890, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 788, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1625, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::PG.train_buffered()\u001b[39m (pid=33654, ip=127.0.0.1, repr=PG)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 224, in train_buffered\n",
      "    result = self.train()\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 679, in train\n",
      "    raise e\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 668, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 283, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 206, in step\n",
      "    step_results = next(self.train_exec_impl)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "    for item in it:\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "    yield ray.get(futures, timeout=timeout)\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=33657, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8af4163470>)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "    return next(self.local_it)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 378, in gen_rollouts\n",
      "    yield self.sample()\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 753, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 233, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 613, in _env_runner\n",
      "    sample_collector=sample_collector,\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 810, in _process_observations\n",
      "    policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "  File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/episode.py\", line 159, in policy_for\n",
      "    raise KeyError(\"policy_mapping_fn returned invalid policy id \"\n",
      "KeyError: \"policy_mapping_fn returned invalid policy id 'agent_0'!\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PG_multienv_FedRL_9c0e2_00000:\n",
      "  date: 2021-12-08_18-58-00\n",
      "  experiment_id: b1dd864eab2848bb88e9c36cd9174a71\n",
      "  hostname: yue0918.local\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 33654\n",
      "  timestamp: 1638961080\n",
      "  trial_id: 9c0e2_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,133\tERROR worker.py:80 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=33671, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7faf311621d0>)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 378, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 753, in sample\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 233, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 613, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     sample_collector=sample_collector,\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 810, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/episode.py\", line 159, in policy_for\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     raise KeyError(\"policy_mapping_fn returned invalid policy id \"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m KeyError: \"policy_mapping_fn returned invalid policy id 'agent_0'!\"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,134\tERROR worker.py:80 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=33668, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc8ef94e358>)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 378, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 753, in sample\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 233, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 613, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     sample_collector=sample_collector,\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 810, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/episode.py\", line 159, in policy_for\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     raise KeyError(\"policy_mapping_fn returned invalid policy id \"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m KeyError: \"policy_mapping_fn returned invalid policy id 'agent_0'!\"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,135\tERROR worker.py:80 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=33665, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbf4d94f208>)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 378, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 753, in sample\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 233, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 613, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     sample_collector=sample_collector,\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 810, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/episode.py\", line 159, in policy_for\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     raise KeyError(\"policy_mapping_fn returned invalid policy id \"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m KeyError: \"policy_mapping_fn returned invalid policy id 'agent_0'!\"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,136\tERROR worker.py:80 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=33660, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ffb37960278>)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 378, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 753, in sample\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 233, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 613, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     sample_collector=sample_collector,\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 810, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/episode.py\", line 159, in policy_for\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     raise KeyError(\"policy_mapping_fn returned invalid policy id \"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m KeyError: \"policy_mapping_fn returned invalid policy id 'agent_0'!\"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,137\tERROR worker.py:80 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=33659, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f885a161400>)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 378, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 753, in sample\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 233, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 613, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     sample_collector=sample_collector,\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 810, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/episode.py\", line 159, in policy_for\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     raise KeyError(\"policy_mapping_fn returned invalid policy id \"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m KeyError: \"policy_mapping_fn returned invalid policy id 'agent_0'!\"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m 2021-12-08 18:58:00,138\tERROR worker.py:80 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=33658, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe7fa1602e8>)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 378, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 753, in sample\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 233, in get_data\n",
      "\u001b[2m\u001b[36m(pid=33657)\u001b[0m 2021-12-08 18:58:00,094\tWARNING deprecation.py:39 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 613, in _env_runner\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     sample_collector=sample_collector,\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 810, in _process_observations\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m   File \"/Users/a10.11.5/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/episode.py\", line 159, in policy_for\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m     raise KeyError(\"policy_mapping_fn returned invalid policy id \"\n",
      "\u001b[2m\u001b[36m(pid=33654)\u001b[0m KeyError: \"policy_mapping_fn returned invalid policy id 'agent_0'!\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 18:58:00 (running for 00:00:42.51)<br>Memory usage on this node: 6.4/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/2.58 GiB heap, 0.0/1.29 GiB objects<br>Result logdir: /Users/a10.11.5/ray_results/CartPole-v0-PG-5<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_multienv_FedRL_9c0e2_00000</td><td>ERROR   </td><td>127.0.0.1:33654</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_multienv_FedRL_9c0e2_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/a10.11.5/ray_results/CartPole-v0-PG-5/PG_multienv_FedRL_9c0e2_00000_0_2021-12-08_18-57-17/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-12-08 18:58:00 (running for 00:00:42.53)<br>Memory usage on this node: 6.4/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/2.58 GiB heap, 0.0/1.29 GiB objects<br>Result logdir: /Users/a10.11.5/ray_results/CartPole-v0-PG-5<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_multienv_FedRL_9c0e2_00000</td><td>ERROR   </td><td>127.0.0.1:33654</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_multienv_FedRL_9c0e2_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/a10.11.5/ray_results/CartPole-v0-PG-5/PG_multienv_FedRL_9c0e2_00000_0_2021-12-08_18-57-17/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33658)\u001b[0m [2021-12-08 18:58:00,784 E 33658 2054927] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "\u001b[2m\u001b[36m(pid=33671)\u001b[0m [2021-12-08 18:58:00,786 E 33671 2055014] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "\u001b[2m\u001b[36m(pid=33659)\u001b[0m [2021-12-08 18:58:00,784 E 33659 2054930] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "\u001b[2m\u001b[36m(pid=33660)\u001b[0m [2021-12-08 18:58:00,784 E 33660 2054945] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PG_multienv_FedRL_9c0e2_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5e0145e52ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    211\u001b[0m })\n\u001b[1;32m    212\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m \u001b[0mfedrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;31m# eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-5e0145e52ce1>\u001b[0m in \u001b[0;36mfedrl\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;34m\"num_gpus\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             },\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcheckpoint_at_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PG_multienv_FedRL_9c0e2_00000])"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from easydict import EasyDict\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray import tune\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "def make_multiagent(args):\n",
    "    class MultiEnv(MultiAgentEnv):\n",
    "        def __init__(self):\n",
    "            self.agents = [gym.make(args.env) for _ in range(args.num_agents)]\n",
    "            self.dones = set()\n",
    "            self.observation_space = self.agents[0].observation_space\n",
    "            self.action_space = self.agents[0].action_space\n",
    "\n",
    "        def reset(self):\n",
    "            self.dones = set()\n",
    "            return {i: a.reset() for i, a in enumerate(self.agents)}\n",
    "\n",
    "        def step(self, action_dict):\n",
    "            obs, rew, done, info = {}, {}, {}, {}\n",
    "            for i, action in action_dict.items():\n",
    "                obs[i], rew[i], done[i], info[i] = self.agents[i].step(action)\n",
    "                if done[i]:\n",
    "                    self.dones.add(i)\n",
    "            done[\"__all__\"] = len(self.dones) == len(self.agents)\n",
    "            return obs, rew, done, info\n",
    "\n",
    "    return MultiEnv\n",
    "\n",
    "def make_fed_env(args):   \n",
    "    FedEnv = make_multiagent(args)\n",
    "    env_name = \"multienv_FedRL\"\n",
    "    register_env(env_name, lambda _: FedEnv())\n",
    "    return env_name\n",
    "\n",
    "def gen_policy_graphs(args):\n",
    "    single_env = gym.make(args.env)\n",
    "    obs_space = single_env.observation_space\n",
    "    act_space = single_env.action_space\n",
    "    policy_graphs = {f'agent_{i}': (None, obs_space, act_space, {}) \n",
    "         for i in range(args.num_agents)}\n",
    "    return policy_graphs\n",
    "\n",
    "def policy_mapping_fn(agent_id):\n",
    "    return f'agent_{agent_id}'\n",
    "def change_weights(weights, i):\n",
    "    \"\"\"\n",
    "    Helper function for FedQ-Learning\n",
    "    \"\"\"\n",
    "    dct = {}\n",
    "    for key, val in weights.items():\n",
    "        # new_key = key\n",
    "        still_here = key[:6]\n",
    "        there_after = key[7:]\n",
    "        # new_key[6] = i\n",
    "        new_key = still_here + str(i) + there_after\n",
    "        dct[new_key] = val\n",
    "    # print(dct.keys())\n",
    "    return dct\n",
    "\n",
    "def synchronize(agent, weights, num_agents):\n",
    "    \"\"\"\n",
    "    Helper function to synchronize weights of the multiagent\n",
    "    \"\"\"\n",
    "    weights_to_set = {f'agent_{i}': weights \n",
    "         for i in range(num_agents)}\n",
    "    # weights_to_set = {f'agent_{i}': change_weights(weights, i) \n",
    "    #    for i in range(num_agents)}\n",
    "    # print(weights_to_set)\n",
    "    agent.set_weights(weights_to_set)\n",
    "\n",
    "def uniform_initialize(agent, num_agents):\n",
    "    \"\"\"\n",
    "    Helper function for uniform initialization\n",
    "    \"\"\"\n",
    "    new_weights = agent.get_weights([\"agent_0\"]).get(\"agent_0\")\n",
    "    # print(new_weights.keys())\n",
    "    synchronize(agent, new_weights, num_agents)\n",
    "\n",
    "def compute_softmax_weighted_avg(weights, alphas, num_agents, temperature=1):\n",
    "    \"\"\"\n",
    "    Helper function to compute weighted avg of weights weighted by alphas\n",
    "    Weights and alphas must have same keys. Uses softmax.\n",
    "    params:\n",
    "        weights - dictionary\n",
    "        alphas - dictionary\n",
    "    returns:\n",
    "        new_weights - array\n",
    "    \"\"\"\n",
    "    def softmax(x, beta=temperature, length=num_agents):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(beta * (x - np.max(x)))\n",
    "        return (e_x / e_x.sum()).reshape(length, 1)\n",
    "    \n",
    "    alpha_vals = np.array(list(alphas.values()))\n",
    "    soft = softmax(alpha_vals)\n",
    "    weight_vals = np.array(list(weights.values()))\n",
    "    new_weights = sum(np.multiply(weight_vals, soft))\n",
    "    return new_weights\n",
    "\n",
    "def reward_weighted_update(agent, result, num_agents):\n",
    "    \"\"\"\n",
    "    Helper function to synchronize weights of multiagent via\n",
    "    reward-weighted avg of weights\n",
    "    \"\"\"\n",
    "    return softmax_reward_weighted_update(agent, result, num_agents, temperature=0)\n",
    "\n",
    "def softmax_reward_weighted_update(agent, result, num_agents, temperature=1):\n",
    "    \"\"\"\n",
    "    Helper function to synchronize weights of multiagent via\n",
    "    softmax reward-weighted avg of weights with specific temperature\n",
    "    \"\"\"\n",
    "    all_weights = agent.get_weights()\n",
    "    policy_reward_mean = result['policy_reward_mean']\n",
    "    episode_reward_mean = result['episode_reward_mean']\n",
    "    if policy_reward_mean:\n",
    "        new_weights = compute_softmax_weighted_avg(all_weights, policy_reward_mean, num_agents, temperature=temperature)\n",
    "        synchronize(agent, new_weights, num_agents)\n",
    "\n",
    "def fed_train(args):\n",
    "    temp_schedule = args.temp_schedule\n",
    "    temperature = temp_schedule[0]\n",
    "    hotter_temp = temp_schedule[1]\n",
    "    temp_shift = temp_schedule[2]\n",
    "    fed_schedule = args.fed_schedule\n",
    "    num_iters = fed_schedule[0]\n",
    "    increased_iters = fed_schedule[1]\n",
    "    fed_shift = fed_schedule[2]\n",
    "    \n",
    "    num_agents = args.num_agents\n",
    "    def fed_learn(info):\n",
    "#       get stuff out of info\n",
    "        result = info[\"result\"]\n",
    "        agent = info[\"trainer\"]\n",
    "        optimizer = agent.optimizer\n",
    "        if result['timesteps_total'] > fed_shift:\n",
    "            num_iters = increased_iters\n",
    "        if result['timesteps_total'] > temp_shift:\n",
    "            temperature = hotter_temp\n",
    "        # correct result reporting\n",
    "        result['episode_reward_mean'] = result['episode_reward_mean']/num_agents\n",
    "        result['episode_reward_max'] = result['episode_reward_max']/num_agents\n",
    "        result['episode_reward_min'] = result['episode_reward_min']/num_agents\n",
    "        result['federated'] = \"No federation\"\n",
    "        if result['training_iteration'] == 1:\n",
    "            uniform_initialize(agent, num_agents)\n",
    "        elif result['training_iteration'] % num_iters == 0:\n",
    "            result['federated'] = f\"Federation with {temperature}\"\n",
    "            # update weights\n",
    "            softmax_reward_weighted_update(agent, result, num_agents, temperature)\n",
    "            # clear buffer, don't want smoothing here\n",
    "            optimizer.episode_history = []\n",
    "    return fed_learn\n",
    "\n",
    "def fedrl(args):\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    policy_graphs = gen_policy_graphs(args)\n",
    "    multienv_name = make_fed_env(args)\n",
    "    callback = fed_train(args)\n",
    "    tune.run(\n",
    "        args.algo,\n",
    "        name=f\"{args.env}-{args.algo}-{args.num_agents}\",\n",
    "        stop={\"episode_reward_mean\": 500},\n",
    "        config={\n",
    "                \"multiagent\": {\n",
    "                    \"policy_graphs\": policy_graphs,\n",
    "                    \"policy_mapping_fn\": tune.function(lambda agent_id: f'agent_{agent_id}'),\n",
    "                },\n",
    "                \"env\": multienv_name,\n",
    "                \"gamma\": 0.99,\n",
    "                # \"lambda\": 0.95,\n",
    "                # \"kl_coeff\": 1.0,\n",
    "                # \"num_sgd_iter\": 32,\n",
    "                # \"lr\": .0003 * args.num_agents,\n",
    "                # \"vf_loss_coeff\": 0.5,\n",
    "                # \"clip_param\": 0.2,\n",
    "                # \"sgd_minibatch_size\": 4096,\n",
    "                \"train_batch_size\": 65536,\n",
    "                # \"grad_clip\": 0.5,\n",
    "                # \"batch_mode\": \"truncate_episodes\",\n",
    "                # \"observation_filter\": \"MeanStdFilter\",\n",
    "                # \"lr\": tune.grid_search(args.lrs),\n",
    "#                 \"simple_optimizer\": True,\n",
    "                \"callbacks\":{\n",
    "                    \"on_train_result\": tune.function(callback),\n",
    "                },\n",
    "                \"num_workers\": args.num_workers,\n",
    "                \"num_gpus\": 0,\n",
    "            },\n",
    "        checkpoint_at_end=False\n",
    "    )\n",
    "\n",
    "\n",
    "args = EasyDict({\n",
    "    'num_agents': 5,\n",
    "    'num_workers': 7,\n",
    "    'fed_schedule': [1, 5, 2e7],\n",
    "    # 'temperatures': [0, 8, 0.5, 4, 2, 1, 16],\n",
    "    'temp_schedule': [0.5, 2, 2e7],\n",
    "    # 'timesteps': 1e7,\n",
    "    # 'lr': 5e-4,\n",
    "    # 'lrs': [5e-5, 5e-4, 5e-3],\n",
    "    # 'episodes': 150,\n",
    "#     'num_iters': 100,\n",
    "    'env': 'CartPole-v0',\n",
    "    'name': 'fed_experiment',\n",
    "    'algo': 'PG',\n",
    "})\n",
    "# train\n",
    "fedrl(args)\n",
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ = [[]]\n",
    "num_models = 3\n",
    "beta = 0.5\n",
    "params = models_[0].named_parameters()\n",
    "dict_params = dict(params)\n",
    "\n",
    "import torch \n",
    "\n",
    "\n",
    "params = models_[0].named_parameters()\n",
    "def model_aggregate(self, weight_accumulator):\n",
    "        for i in range(1, num_models):\n",
    "            for name, param in models_[i].named_parameters():\n",
    "                if name in dict_params:\n",
    "                    mod_eval = [dict_params[name]/dict_params.sum()]\n",
    "                    dict_params[name].data.copy_(param.data*mod_eval[name])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0212cd9fec6ae9fd2bfeec8d60424f1313158b41cfed91a8a0510abbd045b5d2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
